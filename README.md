# Multi-Modal-Interaction
AR-VR-VE for Computer Science Education 1.0

Proposed By: Francisco Ortega Mentor: Francisco Ortega

Team Members: Lukas Borges, Kevin Delamo , Hamilton Chevez, Cristian-Camilo Cabrera-Caranton, Francisco-Javier Lozada, Nicolette-Marie Celli , Filip-Alexander Klepsa, Santiago-Jose Bolivar, Bernardo Blum

This team will look into innovative ways to develop interactions for virtual and augmented reality using equipment such as the HoloLens and the Vive. The particular objective is Computer Science Education.

 
Multi-Modal Interaction
Team Members: Filip Klepsa

Research goals: multi-modal interaction and ASL

Case study:  Building an American Sign Language (ASL) interpreter using Leap Motion Controller. 

This project strives to recognize a physical hand gesture made in American Sign Language (ASL) and convert it to text, which can be imposed onto a Virtual Reality environment. 

 

American Sign Language Interpreter Goals:

1. Recognizing a physical ASL gesture.

2. Constructing a sentence.

3. Correcting the grammar.

4. Sending corrected sentence to an output device.
